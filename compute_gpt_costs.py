# -*- coding: utf-8 -*-
"""Compute_GPT_costs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rY2Nt1jMbhXlvqBz793G7Hm3MM1W46Jn
"""

pip install tiktoken

pip install pandas tiktoken

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import tiktoken

data = pd.read_csv('/content/drive/MyDrive/TupyE-Dataset/balanced_TupyE.csv', index_col = False)

data

data.drop(columns=['Unnamed: 0','id','toxic'], axis=1, inplace=True)

data

# Define a function to convert a row to text
def row_to_text(row):
    return " ".join([str(item) for item in row])

# Function to calculate the number of tokens
def count_tokens(text, model):
    enc = tiktoken.encoding_for_model(model)
    tokens = enc.encode(text)
    return len(tokens)

# Function to calculate cost
def calculate_cost(total_tokens, model):
    model_costs = {
        "davinci": 2.00,
        #"curie": 0.006,
        "babbage": 0.40,
        "ada": 0.10,
        "text-embedding-3-small": 0.02,
        "text-embedding-3-large": 0.13,
    }
    cost_per_1M_tokens = model_costs[model]
    cost = (total_tokens / 1000000) * cost_per_1M_tokens
    return cost

# Load your tabular data into a DataFrame
# Replace 'your_dataset.csv' with your actual dataset file
#df = pd.read_csv('your_dataset.csv')

# Convert each row to text and count tokens
total_tokens = 0
for index, row in data.iterrows():
    text = row_to_text(row)
    tokens = count_tokens(text, model="babbage")
    total_tokens += tokens

# Calculate the total cost
cost = calculate_cost(total_tokens, model="babbage")
print(f"Total cost for processing the dataset with babbage model: ${cost:.2f}")

"""*TupyE-Dataset(balanced) processing costs with GPT*

*   text-embedding-3-large model $0.07

*   text-embedding-3-small model $0.01

*   ada model: $0.07

*   davinci model: $1.34

*   babbage model: $0.27



"""